---
title: "KNOWNET - a guide to data mining and social network analysis"
author: Vladimir Zlatanov
organization: Brunel University
output:
  tufterhandout::html_tufte_handout:
    pandoc_args: ["--toc"]
  tufterhandout::pdf_tufte_handout:
    keep_tex: false
    pandoc_args: ["--toc"]
  word_document:
    pandoc_args: ["-S"]
bibliography: bibliography.bib
csl: harvard-style.csl
---


```{r setup, include=FALSE}
library(ggplot2) # Format R data frames as LaTeX/HTML tables
library(GGally)
library(gridExtra)
library(diagram) 
library(plyr)
library(igraph)  # graph manipulation and anlysis
library(tm)
library(RTextTools)
library(randomNames)
library(wordcloud)
library(knitr)

#opts_chunk$set(cache=TRUE) # caching of document output - could speed up the results
```

Introduction
=========================

This is a step by step guide on analysis and data minining developed for the needs of the KNOWNET project^[More information is avalable on the KNOWNET project website http://www.knownet.org.uk]. Basic familiarity with R [@RCoreTeam2014] would be helpful, but not essential. This document is a [literate program](http://en.wikipedia.org/wiki/Literate_programming) [@knuth1992] written in R and RMarkdown [@RStudio2014]. R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. As it provides a rich variety of tools in order to perform a variety of data mining, knowledge discovery and social network analysis tasks it was chosen as an implementation language. RStudio IDE is a powerful and productive open source user interface for R that works on Windows, MacOS, and Linux.

In order to be able to follow the guide and perform the excercises the following prerequisties ahave to be installed:  

* [R](http://www.r-project.org/) from the R Project [@RCoreTeam2014] 
* [RStudio IDE](http://www.rstudio.com) from RSrudio Inc [@RStudio2012] 
* [Pandoc](http://johnmacfarlane.net/pandoc/)^[Pandoc is a document processor/converterrequired to generetate this guide and documentation]
* [MiKTeX](http://miktex.org/)^[MikTeX is an up-to-date implementation of TeX/LaTeX and related programs for Windows (all current variants)] or another up-to-date TeX/LaTex distribution

All the required programs are available for Windows, Linux and MacOS X, but the help of a systems administrator to install the software may be required.

The aim of this guide is dual - to show how to perform text mining, classification and social network analysis using data captured for the KNOWNET project, and at the same time to ensure the results from the experiments and the analysis explained are reproducible. The reason to use literate programming  [@knuth1992] as the structure of the document is to further the goal of reproducibility - the guide is the program, the resulting pdf or html file is the ooutput of the program. Adding new or different data, will produce new results. Demonstrating reproducible research [@stodden2014; ]  is an important goal.  Literate programming ensures the guide along with the full computational environment used to produce the results in the paper such as the code, data, etc. can be used to reproduce the results and create new work based on it.

This guide is based on the classic Data Mining (DM) and Knowledge Discovery in Databases (KDD) processes [@Fayyad1996], consists of the following steps for exploratory analysis^[The process is repeated until the results are satisfactory]: 
 
  1. Aquisition
  1. Selection 
  2. Pre-processing 
  3. Anonymisation 
  4. Transformation 
  5. Analysis 
  6. Interpretation/Evaluation 

Data Aquisition
=======================

Importing data
-----------------------

The data is aquired from Yammer using web scraping and resides in several comma separated file (csv).

To load the threads data from the corresponding csv file use the *read.csv* function. The first argument is the relative path to the file, the second informs the function that there is no header row containging the names of the columns in the file. The result is data frame^[A data frame in R is used for storing data tables. It is a list of vectors of equal length.]. As the filed does not contain a header, we can name the columns explicitly using the *colnames* function. The order of names must correspond to the order of the columns in the file and the data frame.

```{r da1,tidy = FALSE, prompt=FALSE}
Threads <- read.csv("data/Threads.csv", header=FALSE)

colnames(Threads) <- c("ThreadId", "From", "FromId", "FromType"
                     , "url",  "To", "ToId", "ToType","Message")
```

The same procedure is repeated with the file containing the replies.

```{r da1-1,tidy = FALSE, prompt=FALSE}
Replies <- read.csv("data/Replies.csv", header=FALSE)

colnames(Replies) <- c("ThreadId", "From", "FromId", "FromType"
                     , "url", "To", "ToId", "Message")
```

Data preparation and cleanup
---------------------------------------

The data imported from the two files has to be merged into a *Messages* data frame to have all of the messages in one place. In order to do that, the *Replies* data frame  is augmented with the missing *From* column. 

From observation we know that all the replies are replies to a user, but that information is missing from the table. To rectify this we add a new column to the *Replies* data frame and uniformly assign a value of "user" to every row value in that column.
```{r da2}
Replies$ToType <- "user"
```



Similarly *Threads* can contain rare messages taht are global notifications, visible by anyone, but contain no destination information. It shouldn't change the meaning of the data if we address it to the *All Network* group. To do that, we create an index of all rows in threads which comtain an empty string, and use it to fill the values of the required destination column slots.

```{r da2.1}
index = (Threads$ToType=="")
Threads$To[index] <- "All Network"
Threads$ToId[index] <- "company"
Threads$ToType[index] <- "group"

```

Now we can merge the two data frames row by row by using the *rbind* function.
```{r da2.2}
Messages <- rbind(Threads,Replies)
```

At this stage, the data is unsuitable for publication, as it contains the real names and may contain sensitive information in the message text. To solve the first problem, the procedure is to first create a conversion table, generate random names, add them to the source tables, and replace the real names with the random ones in the *Messages* table.

First, we merge the "From" and "To" columns into a new data frame with one column, which we name *From*.
```{r da3, message=FALSE, results='asis'}
names <- data.frame(levels(factor(c(levels(Messages$From),levels(Messages$To)))))
colnames(names) <- c("From")
```

To ensure that we have a repeatable pseudo-random sequence an explicit seed number is set. Using the same seed number guarantees that the sequence of "random" numbers generated by the computer and used in various routines is the same.

```{r da4}
set.seed(123345)
```

Next, we create an anynomisation translation data frame relating randomly generated names with the real names.
```{r da5}
names$Name <- randomNames(length(levels(names$From)),which.names="first")
```

We crate a cloned copy of the Messages data frame, in which we replace the real names with the previously generated fake ones with the help of the translation data frame.
```{r da6}
AMessages <- Messages;
AMessages$From <- names$Name[AMessages$From]
AMessages$To <- names$Name[AMessages$To]
```

The rest of the file uses the real names as it is easier to relate names with facts. When generating a document for public use uncomment the following line of code assigning the anonymised *AMessage* back to the *Messages* data frame

```{r da7}
Messages <- AMessages;
```

Let's save the *Messages* tables for future use.
```{r da8}
write.csv(Messages, "data/Messages.csv")
write.csv(AMessages, "data/AMessages.csv")
```


The files above cannot be distributed, as they could contain sensitive information in the text of the messages. We can save and possibly make public a subset of the data, removing the message text and url columns. While the result data file cannot be used for text analysis, as the object of analysis is missing, the network data is present and the Social Network analysis can be repeated or expanded on.

```{r da100}
write.csv(AMessages[,c(1:4,6:8)], "data/Messages.safe.csv")
```

Data Exploration
=======================

The purpose of data exploration is to get insights about the data, its structure and content.

Let's look at the first five rows and only the *From*, *To*, and *Message* columns from the *Messages* data frame. The message text is trimmed  for illustration purposes. All of the data can be interavctively explored in RStudio.

```{r de1, results="asis", message=FALSE,warnings = FALSE}
Messages$Text <- strtrim(Messages$Message,30)
kable(Messages[1:5,c("From","To","Text")], format = 'pandoc')

```

Counting and graphing message activity
--------------------------------------

Next let's look at some simple message counts. It can be used to gauge the activity and social dynamics in the network.
```{r de2,  fig.cap="Heatmap of messages between individuals and groups in the social network "}
mcount <- ddply(Messages,.(From,To),nrow)
colnames(mcount) <- c("From", "To", "Count")
base_size <- 10
(ggplot(mcount, aes(To,From)) 
   + geom_tile(aes(fill = Count), colour = "white")
   + scale_fill_gradient(low = "#eeeeee", high = "steelblue")
   + theme_grey(base_size = base_size)
   + theme(
       axis.ticks = element_blank()
     , axis.text.x = element_text(
          size = base_size * 0.8
        , angle = 330
        , hjust = 0
        , colour = "grey50")))
```

Who is most active? With not too many people a plot can highlight the most enthusiastic users of the social network. This does not mean they are people of influence, but indicates that they are worth investigating.
```{r de3,  fig.cap="Messages sent by individuals in the social network" }
(ggplot(Messages, aes(x=From)) + geom_bar() + labs(x = "", y = "")
  + theme_grey(base_size = base_size)
  + theme(
       legend.position = "none"
     , axis.ticks = element_blank()
     , axis.text.x = element_text(
          size = base_size * 0.8
        , angle = 330
        , hjust = 0
        , colour = "grey50")))
```

Similarly, who receives most messages. For individuals it may mean that they are valuable members of the community, depending on context.
```{r de4,  fig.cap="Messages sent to individuals and groups in the social network" }
(ggplot(Messages, aes(x=To)) + geom_bar() + labs(x = "", y = "") 
  + theme_grey(base_size = base_size)
  + theme(
       legend.position = "none"
     , axis.ticks = element_blank()
     , axis.text.x = element_text(
          size = base_size * 0.8
        , angle = 330
        , hjust = 0
        , colour = "grey50")))
```

Message Wordcloud
--------------------

Finally, let's create a word cloud, from the text terms contained in the messages, taking into account the word frequency. First, create a corpus using the *Corpus* and *VectorSource* functions from the *tm* package. 
```{r de5 }
corp <- Corpus(VectorSource(as.vector(Messages$Text)))
```

Next create a term document matrix, which relates the different words and documents, applying a number of different filters like stopwords, tolower, etc...
```{r de6 }
tdm = TermDocumentMatrix(corp,
  control = list(
       removePunctuation = TRUE
     , stopwords = c(stopwords("english"))
     , removeNumbers = TRUE
     , weighting = weightTfIdf
     , tolower = TRUE))

term_matrix <- as.matrix(tdm)
```

Calculate the word frequencies vector *word_freqs* and order the column names according to frequency
```{r de7 }
word_freqs = sort(rowSums(term_matrix), decreasing=TRUE)
dm = data.frame(word=names(word_freqs), freq=word_freqs)
```

Draw a wordcloud based on the most frequesnt words
```{r de8, fig.cap="Wordcloud created from the text of the messages" }
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"), max.words = 20)
```

In order to get a better idea what people are tolking about, we might want tor remove all names from the term document matrix. We will follow the same procedure as before. First create a corpus of names

```{r de9}
namesCorp <- Corpus(VectorSource(as.vector(names$From)))
```

Then, create the term-document matrix.
```{r de10 }
names_tdm = TermDocumentMatrix(namesCorp,
  control = list(
       removePunctuation = TRUE
     , removeNumbers = TRUE
     , tolower = TRUE))

names_matrix <- as.matrix(names_tdm)
```

Extract the terms from the row names of the matrix.

```{r de11}
name_terms <- rownames(names_matrix)
```


Next create a new term document matrix, but this time adding the name terms to the stopwords.
```{r de12}
tdm2 = TermDocumentMatrix(corp,
  control = list(
       removePunctuation = TRUE
     , stopwords = c(stopwords("english"), name_terms)
     , removeNumbers = TRUE
     , tolower = TRUE
     , weighting = weightTfIdf))

term_matrix2 <- as.matrix(tdm2)
```


Calculate the new word frequencies vector *word_freqs2* and order the column names according to frequency
```{r de13 }
word_freqs2 = sort(rowSums(term_matrix2), decreasing=TRUE)
dm2 = data.frame(word=names(word_freqs2), freq=word_freqs2)
```

Draw a wordcloud based on the most frequesnt words
```{r de14, fig.cap="Wordcloud created from the text of the messages with names removed" }
wordcloud(dm2$word, dm2$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"), max.words = 20)
```

If we compare the two worclouds, we can see more content information in the new one. Removing the names give a more "topical" feeling of the text. We could add more stop words in order to improve the information content of the word cloud, i.e. words to be removed from consideration, for example *like*, *also*, *said*, etc... 


Document Classification, Text Analysis
==========================================

For the purposes of the project we have to be able to detect the presence of knowledge artifacts in the body of messages. This is a very broad and consequently vague concept. What is knowledge in this sense? How can we identify knowledge? How can we classify it? What is considered knowledge in the context of the company? What are the knowledge context of the individuals? How does the social network supported by social media help to disseminate or discover knowledge? How does the artifact network, supported by social media help? 

We will work within the KSEM framework[@Helms2009].First, we have to review the data, to be able to identify classes of knowledge carrying artefacts, for example - innovation, optimisation of processes, discussion of results, seeking advice, etc... These are strongly context dependent, they will differ from company to company and network to network, as they depend on what the compaies do, their culture, and other environmental factors.

Next, having identified the classes of knowledge carrying artifacts, we have to perform manual coding of the messages. This can be done in spreadsheets like excel or specialised tools like NVivo. For small numbers of messages, less than a thousand, for example, there is no point in using automatic classification techniques, as the training set will be by nessesity small and the results most likely will contain large errors. For a large corpus of messages, we can use text classification algoritms to help with the coding. We will use the *RTextTools* package [@Jurka2014] as it conveniently provides a number of text mining tools. The resulting predictions tend to improve (nonlinearly) with the size of the reference dataset. Computers with at least 4GB of memory should be able to run RTextTools on medium to large datasets by using the three low-memory algorithms included: general linearized models [@Friedman2010], maximum entropy [@Jurka2012], and support vector machines [@Meyer2014]. In the examples below, we don't use any coding of knowledge artifacts, as the corpus is too small to create meaningful labels at this point. Instead we try to predict the author id *FromId*, from a subset of the data. It is an artificial example, which demonstrates the text classification techniques.

We can use supervised learning algorithms to help with the classification of knowledge carrying artifacts. They are trained on manually coded examples. The supervised learning algorithm attempts to generalise a function or mapping from inputs to outputs which can then be used speculatively to generate an output for previously unseen inputs. We have to split the manually coded data into two sets - a training set and a test set. The training set is used to train the classification algorithm and and the performance is evaluated on the test set. After training the learning algorithms, they can be applied to the unclassified data to automatically predict its coding.

In essence the workflow is[@RTxt2012]: 

* Import hand-coded data into R 
* Remove noise from your data, and create a text corpus the computer can interpret 
* Use algorithm(s) to train a model 
* Test on reference out-of-sample data; establish accuracy criteria 
* Use model to classify virgin data 
* Manually label data that do not meet accuracy criteria 

First, we create a document-term matrix. A number of pre-processing options from the *tm package* [@Feinerer2008] are available at this stage, including stripping whitespace, removing sparse terms, word stemming, and stopword removal for several languages. *removeSparseTerms* is set to .998, to reduce the size of the document-term matrix, running the risk of reducing the accuracy in real-world applications.note that the text column is encapsulated in a *cbind()* data frame, as it allows us to perform supervised learning on multiple columns.

```{r tm1 }
doc_matrix <- create_matrix(
    cbind( Messages["ThreadId"]
       , Messages["From"]
       , Messages["To"]
       , Messages["Text"] )
  , language="english"
  , removeNumbers=FALSE
  , removeSparseTerms=.998
  , stemWords=TRUE
  , weighting=tm::weightTfIdf)
```

The matrix is then partitioned into a container, which is essentially a list of objects that will be fed to the machine learning algorithms in the next step. The output is of class *matrix_container* and includes separate train and test sparse matrices, corresponding vectors of train and test codes, and a character vector of term label names. Looking at the example below, 
*doc_matrix* is the document term matrix created in the previous step.  *trainSize* is the training subset of the data - the documents which will be used to train the learning algorithms.  *testSize* is the test subset of the data, and is going to be used to test the learned models. In the example, the first 60 rows  will be used to train the machine learning model, the next 20 documents will be used to test the model. The *virgin* parameter is set to FALSE because we are still in the evaluation stage and not yet ready to classify virgin documents. When trying to classify with real labels, the second parameter *Messages FromId* should be replaced with the real codes column. Note, in order for the algorithms to work, the codes should be natural numbers.

```{r tm2 }
## nodet: replace Messages$FromId with Code columns
container <- create_container(
    doc_matrix
  , Messages$FromId
  , trainSize=1:60
  , testSize=61:80
  , virgin=FALSE)
```
The train_models() function takes all specified algorithms, to produce an object passable to
classify_models(). 
```{r tm3}
models <- train_models(container, algorithms=c("MAXENT","SVM","GLMNET", "SLDA", "RF", "TREE", "NNET", "BAGGING",  "BOOSTING"))
model <- train_model(container, "MAXENT")
res <- classify_model(container, model)
```

The function classify_models() returns the classified data.
```{r tm4}
results <- classify_models(container, models)

```

The *create_analytics()* function returns a container with four different summaries: by label (e.g., topic), by
algorithm, by document, and ensemble. The summaries help in interpreting the results of the machine learning model predictions.
a more detailed account of the different summaries is given in [@Jurka2014].
```{r tm5}
analytics <- create_analytics(container, results)
```

Use *create_ensembleSummary()* to calculate the recall accuracy and coverage for n ensemble agreement.
```{r tm6, prompt=FALSE, results='asis'}
ensembleSummary <- create_ensembleSummary(analytics@document_summary)
kable(ensembleSummary, format="pandoc")
```



Use *create_precisionRecallSummary()* to creates a summary with precision, recall, and F1 scores for each algorithm broken down by unique label. To better fit the page, we transpose the computed summary, and limit the number of results.
```{r tm7, prompt=FALSE, results='asis'}
prSummary <- create_precisionRecallSummary(container, results)
kable(t(prSummary)[,4:9], format="pandoc")
```

*Add model selection guideline*


```{r tm8-1, prompt=FALSE, results='asis'}
loglikelihood <- function(evidence)
{
  N <- sum(evidence)
  sum(log(abs(evidence - 1e-323)) - log(N))
}

kdiffs <- function(v) {
  function(n) { 2*(n - v) }
}


knr <- function(x) {
  if(x <  0 ) { txt = "Negative"}
  if(x <= 2 ) { txt = "Barely worth a mention"}
  else if(x <= 6 ) { txt = "Positive" }
  else if(x <= 10 ) { txt = "Strong" }
  else { txt = "Very Strong" }
  return(txt)
}

comp_matrix <- function( v ) {
  apply(v,2, kdiffs(v) )
}

logll <- function(v) { t(sort(apply(v,2,loglikelihood))) }

alg_recall <- analytics@algorithm_summary[c(
   "MAXENTROPY_RECALL"
  ,"SVM_RECALL"
  ,"SLDA_RECALL"
  ,"LOGITBOOST_RECALL"
  ,"BAGGING_RECALL"
  ,"FORESTS_RECALL"
  ,"GLMNET_RECALL"
  ,"TREE_RECALL"
  ,"NNETWORK_RECALL"
)]

cm <- comp_matrix(logll(alg_recall))
row.names(cm) <- colnames(cm)

kable(cm, format="pandoc")

```

Social Network Analysis
=======================

This section follows by example the techniques described in the Social Network Analysis Manual [@KNO2013]. It relies on the igraph library [@Csardi2006], which implements a wide array of graph and network analysis routines.

One way to infer a (sub-part) of the social network structure is to use the messages data and build a graph based on who sends a message to whom. First, let's extract a data frame containing only the interesting columns from messages.

```{r ic1}
mgraph <- Messages[,c("From","To")]
```

In *mgraph* we may have repeated (From,To) pairs. Let's count them and add the count as a new column.
```{r ic1_1}
mgraph <- as.data.frame(count(mgraph,c("From","To")))
```


We can create a graph object from the data frame using the *graph.data.frame* method of the *igraph* library.
```{r ic1_2}
gr <- graph.data.frame(mgraph, directed=TRUE)
```

Let's plot the result, notice that we set the random number generator seed for reproducible results.
```{r ic1_3, message=FALSE, results='asis', fig.cap="The social graph derived from message data"}
set.seed(123345)
plot(gr)
```

As the graph may contain loops, let's simplify it by removing any loops by using the *simplify* function.
```{r ic3}
gr <- simplify(gr, remove.loops=TRUE)
```

And let's plot the result.
```{r ic3_1, message=FALSE, results='asis', fig.cap="The simplified graph derived from message data"}
plot(gr)
```
## Network centrality measures

The *igraph* package implements a battery of different network measures usually used in Social Network Analysis. We will demonstrate some of them below for completeness.


The degree is the number of direct ties of an actor (or node), ie how many other nodes are directly connected. We can calculate the degree centralisation measure by using the *centralization.degree* function from the *igraph* library[@Csardi2006].
```{r ncm1}
cD <- centralization.degree(gr, normalized = FALSE)
```

Calculate the normalized degree centralisation of the generated graph
```{r ncm2}
cDn <- centralization.degree(gr, normalized = TRUE)
```

Display the table with the calculated degree centrality measures. The theoretical maximum is a graph level centralization score for a graph with the given number of vertices, using the same parameters. The normalised centrality measure is divided by this number.
```{r ncm4, message=FALSE, results='asis'}
centrality <- rbind(
      "plain" = cD$centralization
    , "normalized" = cDn$centralization
    , "theoretical maximum" = cD$theoretical_max)
centrality <- data.frame(centrality)
colnames(centrality) <- "degree centrality"
kable(centrality, format="pandoc", digits=2)
```


Next will plot the histogram of the distribution of the degrees of the nodes, together with the *mean* and the *median*. In order tro do that, we first have to cast the calculated degrees *cD res* to a data frame.
```{r ncm4_1, tidy = FALSE, fig.width = 4, fig.height = 4, marginfigure = TRUE, fig.cap = "Degree centrality of individual nodes with the mean degree plotted in red and the median in green", prompt=FALSE}
degrees <- cD$res
degrees <- as.data.frame(degrees)
```

Next we build the plot expression piecewise using the appropriate grammar for graphics *ggplot2* [@Wickham2009] combinators for histogram and vertical lines.

```{r fig1, tidy = FALSE, fig.width = 4, fig.height = 4, marginfigure = TRUE, fig.cap = "Degree centrality of individual nodes with the mean degree plotted in red and the median in green", prompt=FALSE}
cdf <- ddply(degrees, "degrees", summarise, degrees.mean=mean(degrees))

( ggplot(degrees, aes(x=degrees))
+ geom_histogram(binwidth = 1) 
+ geom_vline(xintercept=mean(degrees$degrees), color="red")
+ geom_vline(xintercept=median(degrees$degrees), color="green"))
```

Note that the plot is generated as a side effect of an expression, which is a result of the convenient syntax for adding features to a plot using *+* operotor. To prove the point and demonstrate a different useful plot, let's plot plot the densitity of the degree centrality distribution of the graph, overlayed with a histogram of the density.


```{r fig1-1, tidy = FALSE, fig.width = 4, fig.height = 4, marginfigure = TRUE, fig.cap = "Density of the degree centrality distribution, the mean degree in red and the median in green", prompt=FALSE}

( ggplot(degrees, aes(x=degrees)) 
  # Histogram with density instead of count on y-axis
+ geom_histogram(aes(y=..density..),
                 binwidth=1,)
  # Overlay with transparent density plot
+ geom_density(alpha=.5, fill="#DDDDDD")
+ geom_vline(xintercept=mean(degrees$degrees), color="red")
+ geom_vline(xintercept=median(degrees$degrees), color="green"))
```


A different centralisation measure is the *closeness centrality*. It indicates how close is a node from the rest of the network . It represents the ability of a node to reach others[@KNO2013]. Let's calculate both the raw and normalized closeness centralization of the graph.
```{r ncm6}
cC <- centralization.closeness(gr,normalized = FALSE)
cCn <- centralization.closeness(gr,normalized = TRUE)
```

Display the table with the calculated closeness centraility measures
```{r ncm7, message=FALSE, results='asis'}
closeness <- rbind(
      "plain" = cC$centralization
    , "normalized" = cCn$centralization
    , "theoretical maximum" = cC$theoretical_max)
closeness <- data.frame(closeness)
colnames(closeness) <- "closeness centrality"
kable(closeness, format="pandoc", digits=2)
```

Plot the histogram of the closeness for individual nodes of the tree
```{r fig3, fig.width = 4, fig.height = 4, marginfigure = TRUE, fig.cap = "Histogram of the closeness centrality of individual nodes", prompt=FALSE}
closeness <- cC$res
closeness <- as.data.frame(closeness)
( ggplot(closeness, aes(x=closeness))
+ geom_histogram(binwidth = 0.02) 
+ geom_vline(xintercept=mean(closeness$closeness), color="red")
+ geom_vline(xintercept=median(closeness$closeness), color="green")
)
```


The degree of intermediation, or betweenness, indicates how often a node appears in the shortest (or geodesic) section that connects two others. That is, sample when a person is an intermediary between two people of the same group that does not know each other (what we might call "bridge person").[@KNO2013] Let see which are the values for the degree of intermediation in our network. To calculate  the graph betweenness centralization use the *centralization.betweenness*.
```{r ncm8}
cB <- centralization.betweenness (gr, directed = FALSE, normalized = FALSE)
cBn <- centralization.betweenness (gr, directed = FALSE, normalized = TRUE)
```

Display the table with the calculated betweenness centraility measures

```{r ncm9, message=FALSE, results='asis'}
betweenness <- rbind(
      "plain" = cC$centralization
    , "normalized" = cCn$centralization
    , "theoretical maximum" = cC$theoretical_max)
betweenness <- data.frame(betweenness)
colnames(betweenness) <- "betweenness centrality"
kable(betweenness, format="pandoc", digits=2)
```

Plot the histogram of the betweenness centrality of individual nodes, taking care to adjust the bins to sufficiently high value:

```{r  fig5, fig.width = 4, fig.height = 4, marginfigure = TRUE, fig.cap = "Histogram of the betweenness centrality of individual nodes", prompt=FALSE}
betweenness <- cB$res
betweenness <- as.data.frame(betweenness)
( ggplot(betweenness, aes(x=betweenness))
+ geom_histogram(binwidth = 1) 
+ geom_vline(xintercept=mean(betweenness$betweenness), color="red")
+ geom_vline(xintercept=median(betweenness$betweenness), color="green")
)
```


Last, but not least let's calculate the eigenvector centrality scores. They correspond to the values of the first eigenvector of the graph adjacency matrix. They can be interpreted as arising from a reciprocal process in which the centrality of each actor is proportional to the sum of the centralities of those actors to whom he or she is connected. Vertices with high eigenvector centralities are those which are connected to many other vertices which are, in turn, connected to many others (and so on), which implies that the largest values will be obtained by individuals in large cliques or high-density substructures.[@Csardi2006]

```{r ncm13}
cE <- centralization.evcent (gr, directed = FALSE, normalized = FALSE)
cEn <- centralization.evcent (gr, directed = FALSE, normalized = TRUE)
```

Display the table with the calculated eigenvector centrality measures

```{r ncm14, message=FALSE, results='asis'}
evc <- rbind(
      "plain" = cE$centralization
    , "normalized" = cEn$centralization
    , "theoretical maximum" = cE$theoretical_max)
evc <- data.frame(evc)
colnames(evc) <- "eigenvalue centrality"
kable(evc, format="pandoc", digits=2)
```

Plot the histogram of the eigenvector centrality scores of individual nodes:
```{r  fig6, fig.width = 4, fig.height = 4, marginfigure = TRUE, fig.cap = "Histogram of the egenvalue centrality scores of individual nodes", prompt=FALSE}
evc <- cE$vector
evc <- as.data.frame(evc)
( ggplot(evc, aes(x=evc))
+ geom_histogram(binwidth = 0.2) 
+ xlab("eigenvector centrality")
+ geom_vline(xintercept=mean(evc$evc), color="red")
+ geom_vline(xintercept=median(evc$evc), color="green")
)
``` 

And let's plot the density of the eigenvalue centrality scores, notice that a different bin width is used.

```{r  fig6-1, fig.width = 4, fig.height = 4, marginfigure = TRUE, fig.cap = "density of the egenvalue centrality scores of individual nodes", prompt=FALSE}
( ggplot(evc, aes(x=evc)) 
  # Histogram with density instead of count on y-axis
+ geom_histogram(aes(y=..density..),
                 binwidth=0.1,)
+ xlab("eigenvector centrality")
+ geom_density(alpha=.5, fill="#DDDDDD")
+ geom_vline(xintercept=mean(evc$evc), color="red")
+ geom_vline(xintercept=median(evc$evc), color="green"))
```

##  Clustering and cliques

The *transitivity* function from the *igraph* package calculates the clustering coefficient of the graph. It measures the probability that the adjacent vertices of a vertex are connected. A number of transitivity types are implemented, but we will demonstrate only the *global* one, which calculates the ratio of the triangles and the connected triples in the graph. For directed graphs the direction of the edges is ignored.
```{r ncm10}
(clusteringC <- transitivity(gr,type="global"))
```

*largest.cliques* finds all largest cliques in the input graph. A clique is largest if there is no other clique including more vertices.
```{r ncm11,message=FALSE,warnings = FALSE}
grC <- largest.cliques(gr)
```

Let's plot the graph with the cliques coloured in different colours"
```{r  fig7, fig.width=7, fig.height=6, fig.cap="The graph with the cliques coloured in different colours" }
#a list of colours to help with plotting the graphs
colours = c("#E4B9CD","#BF6D92","#8F305B","#5F0930"
           ,"#FFD6CF","#E38F82","#AA4839","#71190B"
           ,"#ACD3B8","#60A876","#2A7E43","#08541F"
           ,"#DFF1C4","#ACD077","#729C34","#41680A"
           ,"#B9E4CD","#6DBF92","#308F5B","#095F30"
           ,"#B9CDE4","#6D92BF","#305B8F","#09305F"          )

#set all vertice colours to white
V(gr)$color <- "white"

#for all cliques set the corresponding vertice colour
for (i in 1:length(grC)) {
  sel <- grC[[i]]
  V(gr)[sel]$color <- colours[i]
}

#plot the result
plot(gr)
```

Similarly *maximal.cliques* finds all maximal cliques in the input graph. A clique in maximal if it cannot be extended to a larger clique. The largest cliques are always maximal, but a maximal clique is not neccessarily the largest.
```{r ncm12}
grC <- maximal.cliques(gr)
```

Let's plot the graph with the cliques coloured in different colours"
```{r  fig8, fig.width=7, fig.height=6, fig.cap="The graph with the cliques coloured in different colours" }
#a list of colours to help with plotting the graphs
colours = c("#E4B9CD","#BF6D92","#8F305B","#5F0930"
           ,"#FFD6CF","#E38F82","#AA4839","#71190B"
           ,"#ACD3B8","#60A876","#2A7E43","#08541F"
           ,"#DFF1C4","#ACD077","#729C34","#41680A"
           ,"#B9E4CD","#6DBF92","#308F5B","#095F30"
           ,"#B9CDE4","#6D92BF","#305B8F","#09305F"          )

#set all vertice colours to white
V(gr)$color <- "white"

#for all cliques set the corresponding vertice colour
for (i in 1:length(grC)) {
  sel <- grC[[i]]
  V(gr)[sel]$color <- colours[i]
}

#plot the result
plot(gr)
```

Next, let's try to find the communities in the generated graph using a model from statistical mechanics called spin-glass. A community is a set of nodes with many edges inside the community and few edges between outside it (i.e. between the community itself and the rest of the graph.)
```{r fig9,  fig.width=7, fig.height=6}
grc <- spinglass.community(gr)
```
Plot the histogram of the community membership distribution
```{r  fig10, fig.cap="Histogram of community membership"}
comm <- grc$membership
comm <- as.data.frame(comm)
colnames(comm)<-"community"
( ggplot(comm, aes(x=comm$community))
+ geom_histogram(binwidth = 1)
)
```

Let's plot the detected communities. 
```{r  fig12, fig.cap="The communities detected in the graph"}
plot(grc,gr,  vertex.size=5)
```

References
--------------------------------------------------------------------
